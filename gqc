#!/usr/bin/env python3

import argparse
import gzip
import os
import re
import sqlite3
import sys
import math

############################################################################
# UTILITIES
############################################################################

def getfp(filename):
	"""Returns a file pointer for reading based on file name"""
	if   filename.endswith('.gz'):
		return gzip.open(filename, 'rt', encoding='ISO-8859-1')
	elif filename == '-':
		return sys.stdin
	else:
		return open(filename)

def readfasta(filename):
	"""Simple fasta file iterator: yields defline, seq"""
	name = None
	seqs = []
	fp = getfp(filename)
	while True:
		line = fp.readline()
		if line == '': break
		line = line.rstrip()
		if line.startswith('>'):
			if len(seqs) > 0:
				seq = ''.join(seqs)
				yield name, seq
				name = line[1:]
				seqs = []
			else:
				name = line[1:]
		else:
			seqs.append(line)
	yield name, ''.join(seqs)
	fp.close()

COMPLEMENT = str.maketrans('ACGTRYMKWSBDHV', 'TGCAYRKMWSVHDB')

def anti(seq):
	"""Returns the reverse complement of a sequence"""
	anti = seq.translate(COMPLEMENT)[::-1]
	return anti

def prob2score(p):
	"""Convert probability to nucleotide-ish log-odds score"""
	if p == 0: return -100
	return math.log2(p/0.25)

############################################################################
# MARKOV MODEL
############################################################################

class Model:

	def __init__(self):
		self.k = None
		self.probs = {}

	def create(self, seqs, beg, end, k=3):
		"""Creates a model from a sequence"""
		self.k = k
		count = {}
		for seq in seqs:
			for i in range(beg+k, len(seq) - end):
				ctx = seq[i-k:i]
				nt = seq[i]
				# initialize with pseudocounts (Laplace smoothing)
				if ctx not in count: count[ctx] = {'A':1, 'C':1, 'G':1, 'T':1}
				if nt not in count[ctx]: count[ctx][nt] = 1
				count[ctx][nt] += 1

		mm = {}
		for kmer in count:
			mm[kmer] = {}
			total = 0
			for nt in count[kmer]: total += count[kmer][nt]
			for nt in count[kmer]: mm[kmer][nt] = count[kmer][nt] / total
		self.probs = mm

	def score(self, seq):
		"""Scores the model against a sequence"""
		score = 0
		k = self.k
		mm = self.probs
		for i in range(k, len(seq)):
			kmer = seq[i-k:i]
			query = seq[i]
			ctx = mm.get(kmer)
			if ctx is None:
				p = 0.25
			else:
				p = ctx.get(query, 0.25)
			# convert to log-odds
			score += prob2score(p)
		return score

	def export_file(self, file):
		"""Exports the Markov model to a named file"""
		mm = self.probs
		with open(file, 'w') as fp:
			fp.write(f'% MM {file} {len(mm)*4}\n')
			for kmer in sorted(mm):
				for v in mm[kmer]:
					fp.write(f'{kmer}{v} {mm[kmer][v]:.6f}\n')
				fp.write('\n')

	def import_file(self, file):
		"""Imports a Markov model from a named file"""
		# expect lines of the form '<kmer><nt> <prob>' (same format as export_file)
		mm = {}
		k = None
		with open(file) as fp:
			for line in fp:
				if line.startswith('%'): continue
				f = line.split()
				if len(f) != 2: continue
				key, val = f[0], float(f[1])
				if len(key) < 2: continue
				kmer, nt = key[:-1], key[-1]
				if k is None: k = len(kmer)
				if kmer not in mm: mm[kmer] = {'A':0, 'C':0, 'G':0, 'T':0}
				mm[kmer][nt] = val
		self.k = k
		self.probs = mm

############################################################################
# LENGTH MODEL
############################################################################

def load_len_model(path):
	"""
	Return (probs_dict, fallback_prob) if file exists, else (None, None).
	probs_dict: {length: probability}
	fallback_prob: smallest_nonzero_prob / 10 or 1e-9 if none
	"""
	try:
		probs = {}
		min_nonzero = None
		with open(path, 'r') as fh:
			first = fh.readline()
			if not first:
				return None, None
			# If header present ("length" first), iterate remaining lines
			if first.lstrip().startswith('length'):
				lines = fh
			else:
				# first line was data, parse it and then continue
				parts = first.strip().split()
				if len(parts) >= 2:
					try:
						l = int(parts[0]); p = float(parts[1])
						probs[l] = p
						if p > 0 and (min_nonzero is None or p < min_nonzero):
							min_nonzero = p
					except Exception:
						# malformed first line â€“ treat as no model
						return None, None
				lines = fh
			for line in lines:
				line = line.strip()
				if not line:
					continue
				parts = line.split()
				if len(parts) < 2:
					continue
				try:
					l = int(parts[0])
					p = float(parts[1])
				except Exception:
					continue
				probs[l] = p
				if p > 0 and (min_nonzero is None or p < min_nonzero):
					min_nonzero = p
		if not probs:
			return None, None
		fallback = (min_nonzero / 10.0) if min_nonzero and min_nonzero > 0.0 else 1e-9
		return probs, fallback
	except FileNotFoundError:
		return None, None

############################################################################
# CLASS DEFINITIONS
############################################################################

class Feature:
	"""represents a genomic feature"""
	def __init__(self, f):
		(sid, src, ft, beg, end, sc, st, ph, fid, pid, info) = f
		self.seqid = sid
		self.source = src
		self.type = ft
		self.beg = beg
		self.end = end
		self.score = sc
		self.strand = st
		self.phase = ph
		self.fid = fid
		self.pid = pid
		self.info = info

	def __str__(self):
		return f'{self.type} {self.beg} {self.end} {self.strand}'

class Gene:
	"""represents a gene"""
	def __init__(self, gf, txs):
		self.seqid = gf.seqid
		self.fid = gf.fid
		self.beg = gf.beg
		self.end = gf.end
		self.strand = gf.strand
		self.score = None # reserved for later
		self.transcripts = txs

	def __str__(self):
		lines = []
		lines.append(f'Gobj: {self.beg} {self.end}')
		for tx in self.transcripts:
			lines.append(f'  {tx}')
			for ex in tx.exons:
				lines.append(f'    {ex}')
		return '\n'.join(lines)

class Transcript:
	def __init__(self, txf):
		# from transcript feature
		self.seqid = txf.seqid
		self.beg = txf.beg
		self.end = txf.end
		self.strand = txf.strand
		self.score = txf.score
		self.fid = txf.fid
		self.pid = txf.pid

		# added to transcript
		self.exons = []
		self.introns = []
		self.cdss = []
		self.utr5s = []
		self.utr3s = []
		self.polya = None

		# calculated later
		self.is_coding = None
		self.is_spliced = None
		self.is_valid = None

		# finalize function here
		self.finalize()

	def finalize(self):
		# calculate self.is_coding
		self.is_coding = True if self.cdss else False
		# calculate self.is_spliced
		self.is_spliced = True if self.introns else False

	def __str__(self):
		return f'TXobj: {self.seqid}:{self.beg}..{self.end}{self.strand}'



############################################################################
# DATABASE FUNCTIONS
############################################################################

def create_database(db, fasta, gff3):
	"""create a new instance of database"""

	if os.path.exists(db): sys.exit(f'aborting: database {db} exists')
	con = sqlite3.connect(db)
	cur = con.cursor()

	# sequences
	cur.execute('CREATE TABLE sequence(seqid TEXT, seq TEXT, info TEXT)')
	cur.execute('CREATE INDEX idx_seq ON sequence (seqid)')
	for header, seq in readfasta(fasta):
		f = header.split()
		if len(f) == 1:
			seqid = f[0]
			info = ''
		else:
			seqid = f[0]
			info = ' '.join(f[1:])
		sql = f'INSERT INTO sequence VALUES ("{seqid}", "{seq}", "{info}")'
		cur.execute(sql)

	# features
	cur.execute('CREATE TABLE feature(seqid TEXT, source TEXT, type TEXT, beg INTEGER, end INTEGER, score NUMERIC, strand TEXT, phase TEXT, fid TEXT, pid TEXT, att TEXT)')
	cur.execute('CREATE INDEX idx_sid ON feature (seqid)')
	cur.execute('CREATE INDEX idx_fid ON feature (fid)')
	cur.execute('CREATE INDEX idx_pid ON feature (pid)')
	cur.execute('CREATE INDEX idx_typ ON feature (type)')
	cur.execute('CREATE INDEX idx_src ON feature (source)')

	grouping = 'ID', 'Parent'

	with getfp(gff3) as fp:
		for line in fp:
			if line.startswith('#'): continue
			f = line.rstrip().split('\t')
			if len(f) != 9: sys.exit('GFF3 requires 9 fields')
			sid, src, typ, beg, end, sc, st, ph, att = f
			info = {k:'' for k in grouping}
			for tv in att.rstrip(';').split(';'):
				tag, value = tv.split('=')
				info[tag] = value

			parent = [] # create duplicate features for multi-parents
			if 'Parent' in info:
				for p in info['Parent'].rstrip(',').split(','):
					parent.append(p)
			else: parent.append('')

			score = 0 if sc == '.' else sc
			fid = info['ID']

			for pid in parent:
				fields = []
				fields.append(f'"{sid}"')
				fields.append(f'"{src}"')
				fields.append(f'"{typ}"')
				fields.append(f'{beg}')
				fields.append(f'{end}')
				fields.append(f'{score}')
				fields.append(f'"{st}"')
				fields.append(f'"{ph}"')
				fields.append(f'"{fid}"')
				fields.append(f'"{pid}"')
				fields.append(f'"{att}"')
				ftxt = ','.join(fields)
				sql = f'INSERT INTO feature VALUES({ftxt})'
				cur.execute(sql)

	con.commit()

def get_seq(db, seqid, beg=None, end=None):
	"""retrieve sequence or subsequence"""

	if not os.path.exists(db): sys.exit(f'aborting: no database {db}')
	con = sqlite3.connect(db)
	cur = con.cursor()

	if beg is None and end is None:
		qseq = 'seq'
	elif end is None:
		qseq = f'substr(seq, {beg})'
	elif beg is None:
		qseq = f'substr(seq, 1, {end})'
	else:
		offset = beg
		length = end - beg + 1
		qseq = f'substr(seq, {offset}, {length})'

	query = f'SELECT {qseq} FROM sequence WHERE seqid="{seqid}"'
	seq = cur.execute(query).fetchone()[0]
	return seq

def get_all_sequences(db):
	"""Yield (seqid, seq) for all entries in the sequence table"""
	if not os.path.exists(db): sys.exit(f'aborting: no database {db}')
	con = sqlite3.connect(db)
	cur = con.cursor()
	for row in cur.execute('SELECT seqid, seq FROM sequence'):
		yield row[0], row[1]
	con.close()

def get_features(db, seqid=None, ftype=None, source=None, fid=None, pid=None):
	"""retrieve features with filters"""

	if not os.path.exists(db): sys.exit(f'aborting: no database {db}')
	con = sqlite3.connect(db)
	cur = con.cursor()

	where = []
	if seqid:  where.append(f'seqid="{seqid}"')
	if ftype:  where.append(f'type="{ftype}"')
	if source: where.append(f'source="{source}"')
	if fid:    where.append(f'fid="{fid}"')
	if pid:    where.append(f'pid="{pid}"')
	query = 'select * from feature';
	if where: query += ' WHERE ' + ' and '.join(where)
	for f in cur.execute(query).fetchall(): yield Feature(f)

def get_seqfeats(db, seqid=None, ftype=None, source=None):
	"""retrive sequences of features with filters"""

	for f in get_features(db, seqid=seqid, ftype=ftype, source=source):
		seq = get_seq(db, f.seqid, beg=f.beg, end=f.end)
		yield seq

def get_genes(db,  seqid=None, source=None, fid=None, pid=None):
	"""retrieve all protein-coding genes"""

	if not os.path.exists(db): sys.exit(f'aborting: no database {db}')
	con = sqlite3.connect(db)
	cur = con.cursor()

	for genef in get_features(db, ftype='gene'):
		# find all transcripts where pid == fid
		txos = []
		for txf in get_features(db, pid=genef.fid):
			txo = Transcript(txf)
			for f in get_features(db, pid=txf.fid):
				if   f.type == 'exon': txo.exons.append(f)
				elif f.type == 'intron': txo.introns.append(f)
				elif f.type == 'CDS': txo.cdss.append(f)
				elif f.type == 'five_prime_UTR': txo.utr5s.append(f)
				elif f.type == 'three_prime_UTR': txo.utr3s.append(f)
				else: sys.exit(f'unknown ftype for tx: {f.type}')
			
			if len(txo.cdss) > 0: txo.is_coding = True
			else: txo.is_coding = False

			txo.is_spliced = True if txo.introns else False

			txos.append(txo)
		yield Gene(genef, txos)


############################################################################
# ACTIONS
############################################################################

def cli_create_database(arg):
	create_database(arg.db, arg.fasta, arg.gff3)

def cli_get_seq(arg):
	seq = get_seq(arg.db, arg.chrom, arg.beg, arg.end)
	print(seq)
	# change to fasta

def cli_get_features(arg):
	for f in get_features(arg.db, arg.chrom, arg.ftype, arg.source):
		print(f)
		# change to gff

def cli_get_seqfeats(arg):
	for s in get_seqfeats(arg.db, arg.chrom, arg.ftype, arg.source):
		print(s)
		# change to fasta

def cli_get_genes(arg):
	for gene in get_genes(arg.db):
		print(gene)


def cli_train_models(arg):
	if arg.verbose:
		print('Verbose mode enabled for training')
		print(f'Using DB: {arg.db}')
		print(f'Force overwrite: {bool(arg.force)}')

	# training the baseline model
	if arg.force or not os.path.exists('baseline.mm'):
		if arg.verbose:
			print('Collecting genome sequences...')

		gen_seqs = []
		for seqid, seq in get_all_sequences(arg.db):
			gen_seqs.append(seq)
			if arg.verbose:
				print(f'  adding sequence {seqid} len={len(seq)}')

		print('Training baseline model...')
		mmbase = Model()
		mmbase.create(seqs=gen_seqs, beg=0, end=0, k=arg.k)
		mmbase.export_file('baseline.mm')
		print('Baseline model trained and saved to baseline.mm')
	else:
		print('Baseline model already exists; use --f to overwrite')

	# defining the features and their corresponding output files 
	feature_configs = [
		('exon',    'exons',   'exon.mm',  'exon.len'),
		('intron',  'introns', 'intron.mm','intron.len'),
		("5' UTR",  'utr5s',   '5utr.mm',  '5utr.len'),
		("3' UTR",  'utr3s',   '3utr.mm',  '3utr.len'),
	]

	# prepare sequence buckets
	sequences = {name: [] for name, *_ in feature_configs}

	# Determine, per-feature, whether each output file needs building
	features_needed = {}
	for name, _, mm_file, len_file in feature_configs:
		need_len = arg.force or not os.path.exists(len_file)
		need_mm = arg.force or not os.path.exists(mm_file)
		features_needed[name] = {
			'len': need_len,
			'mm': need_mm,
			'any': need_len or need_mm
		}

	# If none of the features need anything, skip collection entirely.
	if not any(info['any'] for info in features_needed.values()):
		print('Exon, intron, and UTR models already exist; use --f to overwrite')
	else:
		print('Collecting exon, intron, and UTR sequences...')

		# collect sequences
		for gene in get_genes(arg.db):
			for tx in gene.transcripts:
				if arg.verbose:
					print(f'  processing {tx}')
				for name, attr, _, _ in feature_configs:
					if not features_needed[name]['any']:
						continue
					for feat in getattr(tx, attr):
						seq = get_seq(arg.db, feat.seqid, beg=feat.beg, end=feat.end)
						if feat.strand == '-':
							seq = anti(seq)
						sequences[name].append(seq)
						if arg.verbose:
							print(f'    {name} {feat.beg}-{feat.end} len={len(seq)}')

		if arg.verbose:
			total_utrs = len(sequences["5' UTR"]) + len(sequences["3' UTR"])
			print(
				f'Collected {len(sequences["exon"])} exon sequences, '
				f'{len(sequences["intron"])} intron sequences, '
				f'and {total_utrs} UTR sequences'
			)

	# traing the models for each feature type
	for display_name, _, mm_file, len_file in feature_configs:
		seqs = sequences[display_name]
		total_sequences = len(seqs)

		# length models
		if features_needed[display_name]['len']:
			if total_sequences > 0:
				if arg.verbose:
					print(f'Calculating {display_name} length distribution...')
				length_counts = {}
				for seq in seqs:
					l = len(seq)
					length_counts[l] = length_counts.get(l, 0) + 1
				with open(len_file, 'w') as f:
					f.write('length\tprobability\n')
					for l in sorted(length_counts):
						f.write(f'{l}\t{length_counts[l] / total_sequences:.6f}\n')
				if arg.verbose:
					print(f'  {display_name} length distribution saved to {len_file}')
			else:
				if arg.verbose:
					print(f'  no sequences collected for {display_name}, skipping length model')
		else:
			if arg.verbose:
				print(f'{display_name.capitalize()} length model already exists; use -f to overwrite')

		# Markov models
		if features_needed[display_name]['mm']:
			if total_sequences > 0:
				print(f'Training {display_name} model...')
				mm = Model()
				mm.create(seqs, 0, 0, k=arg.k)
				mm.export_file(mm_file)
				print(f'{display_name.capitalize()} model trained and saved to {mm_file}')
			else:
				print(f'No sequences for {display_name}; skipping {display_name} model')
		else:
			print(f'{display_name.capitalize()} model already exists; use -f to overwrite')

	print('\nModel training complete.')

	
def cli_find_bad_genes(arg):
	badgenes = []

	if arg.verbose:
		print('Verbose mode enabled for scoring')
		print('Loading models: baseline.mm, exon.mm, intron.mm')
		print(f'Using DB: {arg.db}')

	# load markov models
	mmbase = Model();   mmbase.import_file('baseline.mm')
	mmexon = Model();   mmexon.import_file('exon.mm')
	mmintron = Model(); mmintron.import_file('intron.mm')
	mm5utr = Model();   mm5utr.import_file('5utr.mm')
	mm3utr = Model();   mm3utr.import_file('3utr.mm')

	# mapping feature display name to len model filename
	len_files = {
		'exon': 'exon.len',
		'intron': 'intron.len',
		"5' UTR": '5utr.len',
		"3' UTR": '3utr.len'
	}

	# load each .len model
	len_models = {}
	for name, path in len_files.items():
		probs, fallback = load_len_model(path)
		len_models[name] = {
			'probs': probs,
			'fallback': fallback
		}
		if arg.verbose:
			if probs is not None:
				print(f'Loaded length model for {name} from {path} ({len(probs)} lengths, fallback={fallback})')
			else:
				print(f'No length model found for {name} (expected {path}); length-based scoring for this feature will be skipped')

	print('Scoring transcripts...')
	total_tx = 0
	for gene in get_genes(arg.db):
		total_tx += len(gene.transcripts)
		for tx in gene.transcripts:
			# skip non-coding transcripts as before
			if tx.is_coding == False:
				continue

			if arg.verbose:
				print(f'  scoring {tx}')

			# scorers and position counters for overall and per-feature
			total_model_score = total_model_pos = total_baseline_score = total_baseline_pos = 0.0
			exon_model_score = exon_model_pos = exon_baseline_score = exon_baseline_pos = 0.0
			intron_model_score = intron_model_pos = intron_baseline_score = intron_baseline_pos = 0.0
			utrs_model_score = utrs_model_pos = utrs_baseline_score = utrs_baseline_pos = 0.0
			length_score = 0.0

			# feature -> (features_list, model_object, display_name)
			features_and_models = [
				(tx.exons, mmexon, 'exon'),
				(tx.introns, mmintron, 'intron'),
				(tx.utr5s, mm5utr, "5' UTR"),
				(tx.utr3s, mm3utr, "3' UTR"),
			]

			for features, model, name in features_and_models:
				for feat in features:
					seq = get_seq(arg.db, feat.seqid, beg=feat.beg, end=feat.end)
					if feat.strand == '-':
						seq = anti(seq)

					# Markov model score for this sequence
					s = model.score(seq)
					# positions counted as len(seq)-k (floor at 0)
					p = max(0, len(seq) - (getattr(model, 'k', None) or 0))

					# Baseline for this sequence
					s_b = mmbase.score(seq)
					p_b = max(0, len(seq) - (getattr(mmbase, 'k', None) or 0))

					# Aggregate into overall totals
					total_model_score += s
					total_model_pos += p
					total_baseline_score += s_b
					total_baseline_pos += p_b

					# Aggregate into feature-specific totals
					if name == 'exon':
						exon_model_score += s
						exon_model_pos += p
						exon_baseline_score += s_b
						exon_baseline_pos += p_b
					elif name == 'intron':
						intron_model_score += s
						intron_model_pos += p
						intron_baseline_score += s_b
						intron_baseline_pos += p_b
					else:  # UTRs (both 5' and 3') are combined into utrs_*
						utrs_model_score += s
						utrs_model_pos += p
						utrs_baseline_score += s_b
						utrs_baseline_pos += p_b

					# Length-model contribution (if available)
					len_info = len_models.get(name)
					if len_info and len_info['probs'] is not None:
						probs = len_info['probs']
						fallback = len_info['fallback'] or 1e-9
						prob = probs.get(len(seq), fallback)
						if prob <= 0:
							prob = fallback
						length_log = math.log(prob)
						# accumulate across transcript (we'll normalize by total_model_pos later)
						length_score += length_log
					else:
						length_log = 0.0

					if arg.verbose:
						if length_log != 0.0:
							print(f'\t{name} score {s:.3f} baseline {s_b:.3f} length_log {length_log:.4f}')
						else:
							print(f'\t{name} score {s:.3f} baseline {s_b:.3f} (no length model)')

			# If no positions to normalize on, skip this transcript
			if total_model_pos == 0 or total_baseline_pos == 0:
				if arg.verbose:
					print(f'  skipping {tx} (no scoring positions)')
				continue

			# Normalize overall
			model_norm = total_model_score / total_model_pos
			baseline_norm = total_baseline_score / total_baseline_pos

			# Per-feature normalized values (only if positions > 0)
			def safe_norm(score, pos):
				return (score / pos) if pos > 0 else None

			exon_model_norm = safe_norm(exon_model_score, exon_model_pos)
			exon_baseline_norm = safe_norm(exon_baseline_score, exon_baseline_pos)

			intron_model_norm = safe_norm(intron_model_score, intron_model_pos)
			intron_baseline_norm = safe_norm(intron_baseline_score, intron_baseline_pos)

			utrs_model_norm = safe_norm(utrs_model_score, utrs_model_pos)
			utrs_baseline_norm = safe_norm(utrs_baseline_score, utrs_baseline_pos)

			# Length contribution normalized per overall model positions
			# (baseline has no length contribution, so baseline_length_norm = 0)
			length_norm = length_score / total_model_pos  # can be negative
			length_baseline_norm = 0.0
			length_diff = length_baseline_norm - length_norm  # baseline - length

			# Per-feature diffs (baseline - model); None if unavailable
			exon_diff = (exon_baseline_norm - exon_model_norm) if (exon_baseline_norm is not None and exon_model_norm is not None) else None
			intron_diff = (intron_baseline_norm - intron_model_norm) if (intron_baseline_norm is not None and intron_model_norm is not None) else None
			utrs_diff = (utrs_baseline_norm - utrs_model_norm) if (utrs_baseline_norm is not None and utrs_model_norm is not None) else None

			# Overall diff (as before)
			overall_diff = baseline_norm - model_norm

			if arg.verbose:
				print(f'  tx norm: model {model_norm:.4f} baseline {baseline_norm:.4f} overall_diff {overall_diff:.4f}')
				# optionally print per-feature diffs
				if exon_diff is not None: print(f'    exon_diff {exon_diff:.4f}')
				if intron_diff is not None: print(f'    intron_diff {intron_diff:.4f}')
				if utrs_diff is not None: print(f'    utrs_diff {utrs_diff:.4f}')
				print(f'    length_diff {length_diff:.4f}')

			# Append to bad list if overall_diff > 0 (same as before)
			if overall_diff > 0:
				# store the diffs so report can include them
				badgenes.append((
					gene, tx, overall_diff,
					exon_diff, intron_diff, utrs_diff, length_diff
				))

	if arg.verbose:
		print('\nSorting bad genes...')
	# sort by overall difference (largest baseline - model first)
	badgenes.sort(key=lambda tup: tup[2], reverse=True)

	if arg.verbose:
		print(f'Found {len(badgenes)} unlikely transcripts out of {total_tx} total transcripts')

	# write out bad genes to a report file with per-component columns
	with open('badgenes.txt', 'w') as f:
		f.write('Unlikely Transcripts Report\n')
		f.write('============================\n')
		f.write('GeneID\tTranscriptID\tScoreDiff\texon_diff\tintron_diff\tutrs_diff\tlength_diff\n')
		for gene, tx, diff, exon_d, intron_d, utrs_d, length_d in badgenes:
			# format diffs: empty string when None
			fmt = lambda x: f'{x:.4f}' if x is not None else '.'
			f.write(f'{gene.fid}\t{tx.fid}\t{diff:.4f}\t{fmt(exon_d)}\t{fmt(intron_d)}\t{fmt(utrs_d)}\t{fmt(length_d)}\n')

		

############################################################################
# CLI
############################################################################

## parent parser
parser = argparse.ArgumentParser()
parser.add_argument('--db', metavar='<file.db>', default='genome.db',
	help='genome database file [%(default)s]')
sub = parser.add_subparsers(required=True, help='sub-commands')

## create genome database
s1 = sub.add_parser('create',
	help='create a genome database from fasta and gff3 files')
s1.add_argument('--fasta', metavar='<file.fa>', required=True,
	help='genome fasta file, compressed ok')
s1.add_argument('--gff3', metavar='<file.gff3>', required=True,
	help='genome gff3 file, compressed ok')
s1.set_defaults(func=cli_create_database)

## get sequence using coordinates
s2 = sub.add_parser('getseq', help='get sequences or subsequences as FASTA')
s2.add_argument('--chrom', metavar='<name>', required=True,
	help='chromosome name [defaults to all]')
s2.add_argument('--beg', type=int, metavar='<int>', help='beg of subsequence')
s2.add_argument('--end', type=int, metavar='<int>', help='end of subsequence')
s2.add_argument('--anti', action='store_true',
	help='reverse complement the sequence after retrieval')
s2.set_defaults(func=cli_get_seq)

## get features
s3 = sub.add_parser('features', help='get features')
s3.add_argument('--chrom', help='limit to specific chromosome(s)')
s3.add_argument('--ftype', help='limit to specific type(s)')
s3.add_argument('--source', help='limit to specific source(s)')
s3.set_defaults(func=cli_get_features)

## get sequences of features
s4 = sub.add_parser('seqfeats', help='get sequences of features')
s4.add_argument('--chrom', help='limit to specific chromosome(s)')
s4.add_argument('--source', help='limit to specific source(s)')
s4.add_argument('--ftype', help='limit to specific type(s)')
s4.add_argument('--plus', action='store_true',
	help='convert sequence to + strand')
s4.set_defaults(func=cli_get_seqfeats)

## get genes and transcripts
s5 = sub.add_parser('genes', help='get genes')
s5.add_argument('--source')
s5.add_argument('--seq', action='store_true', help='report sequence')
s5.add_argument('--tx', help='send transcripts to named file')
s5.add_argument('--cds', help='send CDS to named file')
s5.set_defaults(func=cli_get_genes)

## train models
s6 = sub.add_parser('models', help='train Markov models')
s6.add_argument('-f', '--force', action='store_true',
	help='force overwrite of existing .mm files')
s6.add_argument('-v', '--verbose', action='store_true',
	help='verbose output')
s6.add_argument('-k', type=int, default=3,
	help='Markov model order [%(default)s]')
s6.set_defaults(func=cli_train_models)

## score markov
s7 = sub.add_parser('findbgs', help='get unlikely transcripts')
s7.add_argument('-v', '--verbose', action='store_true',
	help='verbose output')
s7.set_defaults(func=cli_find_bad_genes)

## execute sub-command
arg = parser.parse_args()
arg.func(arg)